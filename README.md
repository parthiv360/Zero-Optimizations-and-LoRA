# ZeRO Optimizations & LoRA

This repository explores and implements memory-efficient fine-tuning techniques for Large Language Models (LLMs). By combining **Zero Redundancy Optimizer (ZeRO)** techniques with **Low-Rank Adaptation (LoRA)**, this project demonstrates how to scale model training and inference while drastically reducing GPU memory footprints.

## ðŸš€ Overview

Fine-tuning massive parameter models often hits memory walls on standard hardware. This repository provides scripts and experiments leveraging:
* **DeepSpeed ZeRO (Stages 1, 2, and 3):** To partition optimizer states, gradients, and model parameters across distributed devices.
* **LoRA (Low-Rank Adaptation):** To freeze pre-trained model weights and inject trainable rank decomposition matrices into each layer of the Transformer architecture, significantly reducing the number of trainable parameters.

## ðŸ“‚ Repository Structure

```text
â”œâ”€â”€ config/                   # Config files for different ZeRO Optimization levels
â”‚   â”œâ”€â”€ ds_z1.yaml     
â”‚   â”œâ”€â”€ ds_z2.yaml      
â”‚   â””â”€â”€ ds_z3.yaml             
â”œâ”€â”€ scripts/                  # Execution scripts for various training/profiling runs
â”‚   â”œâ”€â”€ baseline.sh
â”‚   â”œâ”€â”€ lora.sh
â”‚   â””â”€â”€ zero.sh
â”œâ”€â”€ lora.py                   # Core script for applying LoRA
â”œâ”€â”€ main.py                   # Main execution/training script
â””â”€â”€ README.md                 # Project documentation

```

## ðŸ“š References
If you want to dive deeper into the theory behind these optimization techniques, check out the original research papers:
* **ZeRO**: [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)
* **LORA**: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
